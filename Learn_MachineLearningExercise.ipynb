{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [Basic Data Exploration](https://www.kaggle.com/kernels/fork/1258954)\n\nThis exercise will test your ability to read a data file and understand statistics about the data.\n\nIn later exercises, you will apply techniques to filter the data, build a machine learning model, and iteratively improve your model.\n\nThe course examples use data from Melbourne. To ensure you can apply these techniques on your own, you will have to apply them to a new dataset (with house prices from Iowa).\n\nThe exercises use a \"notebook\" coding environment.  In case you are unfamiliar with notebooks, we have a [90-second intro video](https://www.youtube.com/watch?v=4C2qMnaIKL4).\n\n# Exercises\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Loading Data\nRead the Iowa data file into a Pandas DataFrame called `home_data`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\n# Fill in the line bellow to read the file into a variable home_data\nhome_data = pd.read_csv(iowa_file_path)\n\n# Print summary statistics in next line\nhome_data.describe()","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\ncount  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \nmean    730.500000    56.897260    70.049958   10516.828082     6.099315   \nstd     421.610009    42.300571    24.284752    9981.264932     1.382997   \nmin       1.000000    20.000000    21.000000    1300.000000     1.000000   \n25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \nmax    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n\n       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\ncount  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \nmean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \nstd       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \nmin       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \nmax       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n\n        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\ncount  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \nmean     94.244521    46.660274      21.954110     3.409589    15.060959   \nstd     125.338794    66.256028      61.119149    29.317331    55.757415   \nmin       0.000000     0.000000       0.000000     0.000000     0.000000   \n25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n75%     168.000000    68.000000       0.000000     0.000000     0.000000   \nmax     857.000000   547.000000     552.000000   508.000000   480.000000   \n\n          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \ncount  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \nmean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \nstd      40.177307    496.123024     2.703626     1.328095   79442.502883  \nmin       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \nmax     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n\n[8 rows x 38 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>...</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1201.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1452.000000</td>\n      <td>1460.000000</td>\n      <td>...</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n      <td>1460.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>730.500000</td>\n      <td>56.897260</td>\n      <td>70.049958</td>\n      <td>10516.828082</td>\n      <td>6.099315</td>\n      <td>5.575342</td>\n      <td>1971.267808</td>\n      <td>1984.865753</td>\n      <td>103.685262</td>\n      <td>443.639726</td>\n      <td>...</td>\n      <td>94.244521</td>\n      <td>46.660274</td>\n      <td>21.954110</td>\n      <td>3.409589</td>\n      <td>15.060959</td>\n      <td>2.758904</td>\n      <td>43.489041</td>\n      <td>6.321918</td>\n      <td>2007.815753</td>\n      <td>180921.195890</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>421.610009</td>\n      <td>42.300571</td>\n      <td>24.284752</td>\n      <td>9981.264932</td>\n      <td>1.382997</td>\n      <td>1.112799</td>\n      <td>30.202904</td>\n      <td>20.645407</td>\n      <td>181.066207</td>\n      <td>456.098091</td>\n      <td>...</td>\n      <td>125.338794</td>\n      <td>66.256028</td>\n      <td>61.119149</td>\n      <td>29.317331</td>\n      <td>55.757415</td>\n      <td>40.177307</td>\n      <td>496.123024</td>\n      <td>2.703626</td>\n      <td>1.328095</td>\n      <td>79442.502883</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>20.000000</td>\n      <td>21.000000</td>\n      <td>1300.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1872.000000</td>\n      <td>1950.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2006.000000</td>\n      <td>34900.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>365.750000</td>\n      <td>20.000000</td>\n      <td>59.000000</td>\n      <td>7553.500000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>1954.000000</td>\n      <td>1967.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>2007.000000</td>\n      <td>129975.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>730.500000</td>\n      <td>50.000000</td>\n      <td>69.000000</td>\n      <td>9478.500000</td>\n      <td>6.000000</td>\n      <td>5.000000</td>\n      <td>1973.000000</td>\n      <td>1994.000000</td>\n      <td>0.000000</td>\n      <td>383.500000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n      <td>2008.000000</td>\n      <td>163000.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1095.250000</td>\n      <td>70.000000</td>\n      <td>80.000000</td>\n      <td>11601.500000</td>\n      <td>7.000000</td>\n      <td>6.000000</td>\n      <td>2000.000000</td>\n      <td>2004.000000</td>\n      <td>166.000000</td>\n      <td>712.250000</td>\n      <td>...</td>\n      <td>168.000000</td>\n      <td>68.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>2009.000000</td>\n      <td>214000.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1460.000000</td>\n      <td>190.000000</td>\n      <td>313.000000</td>\n      <td>215245.000000</td>\n      <td>10.000000</td>\n      <td>9.000000</td>\n      <td>2010.000000</td>\n      <td>2010.000000</td>\n      <td>1600.000000</td>\n      <td>5644.000000</td>\n      <td>...</td>\n      <td>857.000000</td>\n      <td>547.000000</td>\n      <td>552.000000</td>\n      <td>508.000000</td>\n      <td>480.000000</td>\n      <td>738.000000</td>\n      <td>15500.000000</td>\n      <td>12.000000</td>\n      <td>2010.000000</td>\n      <td>755000.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 38 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is the average lot size (rounded to nearest integer)?\navg_lot_size = 10517\n\n# As of today, how old is the newest home (current year - the date in which it was built)\nnewest_home_age = 2019-2010\n","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Think About Your Data\n\nThe newest house in your data isn't that new.  A few potential explanations for this:\n1. They haven't built new houses where this data was collected.\n1. The data was collected a long time ago. Houses built after the data publication wouldn't show up.\n\nIf the reason is explanation #1 above, does that affect your trust in the model you build with this data? What about if it is reason #2?\n\nHow could you dig into the data to see which explanation is more plausible?\n\nCheck out this **[discussion thread](https://www.kaggle.com/learn-forum/60581)** to see what others think or to add your ideas.\n\n# Keep Going\n\nYou are ready for **[Your First Machine Learning Model](https://www.kaggle.com/dansbecker/your-first-machine-learning-model).**\n\n---\n**[Machine Learning Course Home Page](https://www.kaggle.com/learn/machine-learning)**\n\n"},{"metadata":{},"cell_type":"markdown","source":"# [Your First Machine Learning Model](https://www.kaggle.com/kernels/fork/400771)\n\n## Recap\nSo far, you have loaded your data and reviewed it with the following code. Run this cell to set up your coding environment where the previous step left off."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code you have previously used to load data\nimport pandas as pd\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exercises\n\n## Step 1: Specify Prediction Target\nSelect the target variable, which corresponds to the sales price. Save this to a new variable called `y`. You'll need to print a list of the columns to find the name of the column you need.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the list of columns in the dataset to find the name of the prediction target\nhome_data.columns","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition', 'SalePrice'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = home_data.SalePrice","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Create X\nNow you will create a DataFrame called `X` holding the predictive features.\n\nSince you want only some columns from the original data, you'll first create a list with the names of the columns you want in `X`.\n\nYou'll use just the following columns in the list (you can copy and paste the whole list to save some typing, though you'll still need to add quotes):\n    * LotArea\n    * YearBuilt\n    * 1stFlrSF\n    * 2ndFlrSF\n    * FullBath\n    * BedroomAbvGr\n    * TotRmsAbvGrd\n\nAfter you've created that list of features, use it to create the DataFrame that you'll use to fit the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the list of features below\nfeature_names = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\n\n# select data corresponding to features in feature_names\nX = home_data[feature_names]\n","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Review Data\nBefore building a model, take a quick look at **X** to verify it looks sensible"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Review data\n# print description or statistics from X\nprint(X.describe())\n\n# print the top few lines\nprint(X.head())","execution_count":27,"outputs":[{"output_type":"stream","text":"             LotArea    YearBuilt     1stFlrSF     2ndFlrSF     FullBath  \\\ncount    1460.000000  1460.000000  1460.000000  1460.000000  1460.000000   \nmean    10516.828082  1971.267808  1162.626712   346.992466     1.565068   \nstd      9981.264932    30.202904   386.587738   436.528436     0.550916   \nmin      1300.000000  1872.000000   334.000000     0.000000     0.000000   \n25%      7553.500000  1954.000000   882.000000     0.000000     1.000000   \n50%      9478.500000  1973.000000  1087.000000     0.000000     2.000000   \n75%     11601.500000  2000.000000  1391.250000   728.000000     2.000000   \nmax    215245.000000  2010.000000  4692.000000  2065.000000     3.000000   \n\n       BedroomAbvGr  TotRmsAbvGrd  \ncount   1460.000000   1460.000000  \nmean       2.866438      6.517808  \nstd        0.815778      1.625393  \nmin        0.000000      2.000000  \n25%        2.000000      5.000000  \n50%        3.000000      6.000000  \n75%        3.000000      7.000000  \nmax        8.000000     14.000000  \n   LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n0     8450       2003       856       854         2             3   \n1     9600       1976      1262         0         2             3   \n2    11250       2001       920       866         2             3   \n3     9550       1915       961       756         1             3   \n4    14260       2000      1145      1053         2             4   \n\n   TotRmsAbvGrd  \n0             8  \n1             6  \n2             6  \n3             7  \n4             9  \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Specify and Fit Model\nCreate a `DecisionTreeRegressor` and save it iowa_model. Ensure you've done the relevant import from sklearn to run this command.\n\nThen fit the model you just created using the data in `X` and `y` that you saved above."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n#specify the model. \n#For model reproducibility, set a numeric value for random_state when specifying the model\niowa_model = DecisionTreeRegressor(random_state=0)\n\n# Fit the model\niowa_model.fit(X,y)\n","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=0, splitter='best')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Make Predictions\nMake predictions with the model's `predict` command using `X` as the data. Save the results to a variable called `predictions`."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = iowa_model.predict(X)\nprint(predictions)","execution_count":29,"outputs":[{"output_type":"stream","text":"[208500. 181500. 223500. ... 266500. 142125. 147500.]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Think About Your Results\n\nUse the `head` method to compare the top few predictions to the actual home values (in `y`) for those same homes. Anything surprising?\n\nYou'll understand why this happened if you keep going.\n\n## Keep Going\nYou've built a decision tree model.  It's natural to ask how accurate the model's predictions will be and how you can improve that. Learn how to do that with **[Model Validation](https://www.kaggle.com/dansbecker/model-validation)**.\n\n---\n**[Course Home Page](https://www.kaggle.com/learn/machine-learning)**\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# [Model Validation](https://www.kaggle.com/kernels/fork/1259097)\n\n## Recap\nYou've built a model. In this exercise you will test how good your model is.\n\nRun the cell below to set up your coding environment where the previous exercise left off.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code you have previously used to load data\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\ny = home_data.SalePrice\nfeature_columns = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[feature_columns]\n\n# Specify Model\niowa_model = DecisionTreeRegressor()\n# Fit Model\niowa_model.fit(X, y)\n\nprint(\"First in-sample predictions:\", iowa_model.predict(X.head()))\nprint(\"Actual target values for those homes:\", y.head().tolist())\n","execution_count":30,"outputs":[{"output_type":"stream","text":"First in-sample predictions: [208500. 181500. 223500. 140000. 250000.]\nActual target values for those homes: [208500, 181500, 223500, 140000, 250000]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Exercises\n\n## Step 1: Split Your Data\nUse the `train_test_split` function to split up your data.\n\nGive it the argument `random_state=1` so the `check` functions know what to expect when verifying your code.\n\nRecall, your features are loaded in the DataFrame **X** and your target is loaded in **y**.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,random_state=1)","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Specify and Fit the Model\n\nCreate a `DecisionTreeRegressor` model and fit it to the relevant data.\nSet `random_state` to 1 again when creating the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify the model\niowa_model = DecisionTreeRegressor(random_state=1)\n\n# Fit iowa_model with the training data.\niowa_model.fit(train_X,train_y)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=1, splitter='best')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Make Predictions with Validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_predictions = iowa_model.predict(val_X)","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspect your predictions and actual values from validation data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the top few validation predictions\nprint(iowa_model.predict(val_X.head()))\n\n# print the top few actual prices from validation data\nprint(val_y.head().tolist())","execution_count":34,"outputs":[{"output_type":"stream","text":"[186500. 184000. 130000.  92000. 164500.]\n[231500, 179500, 122000, 84500, 142000]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"What do you notice that is different from what you saw with in-sample predictions (which are printed after the top code cell in this page).\n\nDo you remember why validation predictions differ from in-sample (or training) predictions? This is an important idea from the last lesson.\n\n## Step 4: Calculate the Mean Absolute Error in Validation Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\nval_mae = mean_absolute_error(val_y,val_predictions)\n\nprint(\"MAE: \",val_mae)","execution_count":35,"outputs":[{"output_type":"stream","text":"MAE:  29652.931506849316\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Is that MAE good?  There isn't a general rule for what values are good that applies across applications. But you'll see how to use (and improve) this number in the next step.\n\n# Keep Going\n\nYou are ready for **[Underfitting and Overfitting](https://www.kaggle.com/dansbecker/underfitting-and-overfitting).**\n"},{"metadata":{},"cell_type":"markdown","source":"# [Underfitting and Overfitting](https://www.kaggle.com/kernels/fork/1259126)\n\n## Recap\nYou've built your first model, and now it's time to optimize the size of the tree to make better predictions. Run this cell to set up your coding environment where the previous step left off."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n\n# Create target object and call it y\ny = home_data.SalePrice\n\n# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,random_state=1)\n\n# Specify Model\niowa_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model\niowa_model.fit(train_X,train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE: {:,.0f}\".format(val_mae))\nprint(val_mae)","execution_count":36,"outputs":[{"output_type":"stream","text":"Validation MAE: 29,653\n29652.931506849316\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Exercises\nYou could write the function `get_mae` yourself. For now, we'll supply it. This is the same function you read about in the previous lesson. Just run the cell below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0)\n    model.fit(train_X, train_y)\n    pred_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, pred_val)\n    return (mae)","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Compare Different Tree Sizes\nWrite a loop that tries the following values for *max_leaf_nodes* from a set of possible values.\n\nCall the *get_mae* function on each value of max_leaf_nodes. Store the output in some way that allows you to select the value of `max_leaf_nodes` that gives the most accurate model on your data."},{"metadata":{"trusted":true},"cell_type":"code","source":"candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n\nmae_list = []\n\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    mae_leaf = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    \n    print(\"size: \",max_leaf_nodes,\" \\t MAE: \",mae_leaf)\n    \n    mae_list.append(mae_leaf)\n    \n# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\nbest_tree_size = candidate_max_leaf_nodes[mae_list.index(min(mae_list))]\n\nprint(best_tree_size)","execution_count":38,"outputs":[{"output_type":"stream","text":"size:  5  \t MAE:  35044.51299744237\nsize:  25  \t MAE:  29016.41319191076\nsize:  50  \t MAE:  27405.930473214907\nsize:  100  \t MAE:  27282.50803885739\nsize:  250  \t MAE:  27893.822225701646\nsize:  500  \t MAE:  29454.18598068598\n100\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Fit Model Using All Data\nYou know the best tree size. If you were going to deploy this model in practice, you would make it even more accurate by using all of the data and keeping that tree size.  That is, you don't need to hold out the validation data now that you've made all your modeling decisions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in argument to make optimal size and uncomment\nfinal_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n\n# Fit the final model\nfinal_model.fit(X,y)","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n                      max_leaf_nodes=100, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=1, splitter='best')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"You've tuned this model and improved your results. But we are still using Decision Tree models, which are not very sophisticated by modern machine learning standards. In the next step you will learn to use Random Forests to improve your models even more.\n\n# Keep Going\n\nYou are ready for **[Random Forests](https://www.kaggle.com/dansbecker/random-forests).**\n"},{"metadata":{},"cell_type":"markdown","source":"# [Random Forest](https://www.kaggle.com/kernels/fork/1259186)\n## Recap\nHere's the code you've written so far."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics  import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n\n# Create target object and call it y\ny = home_data.SalePrice\n# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Specify Model\niowa_model = DecisionTreeRegressor(random_state=1)\n# Fit model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions,val_y)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Using best value for max_leaf_nodes\niowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\niowa_model.fit(train_X, train_y)\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))","execution_count":40,"outputs":[{"output_type":"stream","text":"Validation MAE when not specifying max_leaf_nodes: 29,653\nValidation MAE for best value of max_leaf_nodes: 27,283\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"home_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exercises\nData science isn't always this easy. But replacing the decision tree with a Random Forest is going to be an easy win."},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Use a Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Define the model. Set random_state to 1\nrf_model = RandomForestRegressor(random_state=1)\n\n# Fit your model\nrf_model.fit(train_X, train_y)\n\n# Calculate the mean absolute error of your Random Forest model on the validation data\nrf_val_mae = mean_absolute_error(rf_model.predict(val_X),val_y)\n\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))","execution_count":41,"outputs":[{"output_type":"stream","text":"Validation MAE for Random Forest Model: 22762.42931506849\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"So far, you have followed specific instructions at each step of your project. This helped learn key ideas and build your first model, but now you know enough to try things on your own. \n\nMachine Learning competitions are a great way to try your own ideas and learn more as you independently navigate a machine learning project. \n"},{"metadata":{},"cell_type":"markdown","source":"# Keep Going\n\nYou are ready for **[Machine Learning Competitions](https://www.kaggle.com/kernels/fork/1259198).**\n"},{"metadata":{},"cell_type":"markdown","source":"# [Exercise: Machine Learning Competitions](https://www.kaggle.com/kernels/fork/1259198)\n\n# Introduction\nMachine learning competitions are a great way to improve your data science skills and measure your progress. \n\nIn this exercise, you will create and submit predictions for a Kaggle competition. You can then improve your model (e.g. by adding features) to improve and see how you stack up to others taking this micro-course.\n\nThe steps in this notebook are:\n1. Build a Random Forest model with all of your data (**X** and **y**)\n2. Read in the \"test\" data, which doesn't include values for the target.  Predict home values in the test data with your Random Forest model.\n3. Submit those predictions to the competition and see your score.\n4. Optionally, come back to see if you can improve your model by adding features or changing your model. Then you can resubmit to see how that stacks up on the competition leaderboard.\n\n## Recap\nHere's the code you've written so far. Start by running it again.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Path of the file to read. We changed the directory structure to simplify submitting to a competition\niowa_file_path = '../input/house-prices-advanced-regression-techniques/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n\n# Create target object and call it y\ny = home_data.SalePrice\n\n# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Specify Model\niowa_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Using best value for max_leaf_nodes\niowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\niowa_model.fit(train_X, train_y)\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Define the model. Set random_state to 1\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))","execution_count":42,"outputs":[{"output_type":"stream","text":"Validation MAE when not specifying max_leaf_nodes: 29,653\nValidation MAE for best value of max_leaf_nodes: 27,283\nValidation MAE for Random Forest Model: 22,762\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Creating a Model For the Competition\n\nBuild a Random Forest model and train it on all of **X** and **y**.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# To imporve accuracym create a new Random Forest model which you will train on all training data\nrf_model_on_full_data = RandomForestRegressor(random_state=1)\n\n# fir rf_model_on_full_data on all data from the training data\nrf_model_on_full_data.fit(X,y)\n\nrf_predictions_on_full_data = rf_model_on_full_data.predict(X)\n\nprint(rf_predictions_on_full_data)","execution_count":43,"outputs":[{"output_type":"stream","text":"[202750.  175780.  222200.  ... 253180.  129937.5 151650. ]\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Make Predictions\nRead the file of \"test\" data. And apply your model to make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# path to file you will use for predictions\ntest_data_path = '../input/house-prices-advanced-regression-techniques/test.csv'\n\n# read test data file using pandas\ntest_data = pd.read_csv(test_data_path)\n\n# create test_X which comes from test_data but includes only the columns you used for prediction\n# The list of columns is stored in a variable called features\n\n#print(test_data)\n#print(features)\n\ntest_X = test_data[features]\n\ntest_preds = rf_model_on_full_data.predict(test_X)\nprint(test_preds)\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv',index=False)","execution_count":44,"outputs":[{"output_type":"stream","text":"[112945.  149770.  178100.  ... 141674.1 130350.  228830. ]\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Test Your Work\nAfter filling in the code above:\n1. Click the **Commit and Run** button. \n2. After your code has finished running, click the small double brackets **<<** in the upper left of your screen.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n3. Go to the output tab at top of your screen. Select the button to submit your file to the competition.  \n4. If you want to keep working to improve your model, select the edit button. Then you can change your model and repeat the process.\n\nCongratulations, you've started competing in Machine Learning competitions.\n\n# Continuing Your Progress\nThere are many ways to improve your model, and **experimenting is a great way to learn at this point.**\n\nThe best way to improve your model is to add features.  Look at the list of columns and think about what might affect home prices.  Some features will cause errors because of issues like missing values or non-numeric data types. \n\nLevel 2 of this micro-course will teach you how to handle these types of features. You will also learn to use **xgboost**, a technique giving even better accuracy than Random Forest.\n\n\n# Other Micro-Courses\nThe **[Pandas Micro-Course](https://kaggle.com/Learn/Pandas)** will give you the data manipulation skills to quickly go from conceptual idea to implementation in your data science projects. \n\nYou are also ready for the **[Deep Learning](https://kaggle.com/Learn/Deep-Learning)** micro-course, where you will build models with better-than-human level performance at computer vision tasks."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}